{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_441_RNN_and_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donw385/DS-Unit-4-Sprint-4-Deep-Learning/blob/master/module1-rnn-and-lstm/LS_DS_441_RNN_and_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_IizNKWLomoA"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 4 Lesson 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "## _aka_ PREDICTING THE FUTURE!\n",
        "\n",
        "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
        "\n",
        "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
        "\n",
        "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
        "\n",
        "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5_m0hJ4uCzHz"
      },
      "source": [
        "## Time series with plain old regression\n",
        "\n",
        "Recurrences are fancy, and we'll get to those later - let's start with something simple. Regression can handle time series just fine if you just set them up correctly - let's try some made-up stock data. And to make it, let's use a few list comprehensions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GkJUFfsgnqr_",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from random import random\n",
        "days = np.array((range(28)))\n",
        "stock_quotes = np.array([random() + day * random() for day in days])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y-ORgKGNBOcb",
        "outputId": "b1be3458-a106-4dc4-81ef-bce6e5f70749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "stock_quotes"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.48324189,  1.28552801,  1.07749673,  2.20101578,  2.34035905,\n",
              "        5.219344  ,  4.86620928,  2.13885581,  2.21754185,  0.31250494,\n",
              "        2.52825716,  4.75169397,  7.22776656,  4.13952298,  6.37975307,\n",
              "       12.91312597,  0.59750793, 16.64426664, 18.55253007, 19.8862888 ,\n",
              "        9.11683515,  4.30227898, 13.36109653,  6.8023035 , 20.89084042,\n",
              "       13.54138867, 12.85526008, 16.30275125])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X3lR2wGvBx3a"
      },
      "source": [
        "Let's take a look with a scatter plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pVUTC2tmBSIq",
        "outputId": "f53209fd-29dc-4738-9462-c58b1a488602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "from matplotlib.pyplot import scatter\n",
        "scatter(days, stock_quotes);"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOgAEQaADQBAEOgAEQaADQBC/BckLLVpbTOp5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hgD4q-T_B0jd"
      },
      "source": [
        "Looks pretty linear, let's try a simple OLS regression.\n",
        "\n",
        "First, these need to be NumPy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A3Q0MrnUBXAl",
        "colab": {}
      },
      "source": [
        "days = days.reshape(-1, 1)  # X needs to be column vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI-qC_j3LbCQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "08b867a1-4156-42e3-94c4-bf833c7b1aa5"
      },
      "source": [
        "days"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0],\n",
              "       [ 1],\n",
              "       [ 2],\n",
              "       [ 3],\n",
              "       [ 4],\n",
              "       [ 5],\n",
              "       [ 6],\n",
              "       [ 7],\n",
              "       [ 8],\n",
              "       [ 9],\n",
              "       [10],\n",
              "       [11],\n",
              "       [12],\n",
              "       [13],\n",
              "       [14],\n",
              "       [15],\n",
              "       [16],\n",
              "       [17],\n",
              "       [18],\n",
              "       [19],\n",
              "       [20],\n",
              "       [21],\n",
              "       [22],\n",
              "       [23],\n",
              "       [24],\n",
              "       [25],\n",
              "       [26],\n",
              "       [27]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vqr0SHOnB5yR"
      },
      "source": [
        "Now let's use good old `scikit-learn` and linear regression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PqyHxgFvBYl5",
        "outputId": "f9f6aa90-343b-451d-8b36-132f87225fd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "ols_stocks = LinearRegression()\n",
        "ols_stocks.fit(days, stock_quotes)\n",
        "ols_stocks.score(days, stock_quotes)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5512958772378235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KlU0mr-KB_Yk"
      },
      "source": [
        "That seems to work pretty well, but real stocks don't work like this.\n",
        "\n",
        "Let's make *slightly* more realistic data that depends on more than just time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-FV1Emb2BuLz",
        "colab": {}
      },
      "source": [
        "# Not everything is best as a comprehension\n",
        "stock_data = np.empty([len(days), 4])\n",
        "for day in days:\n",
        "  asset = random()\n",
        "  liability = random()\n",
        "  quote = random() + ((day * random()) + (20 * asset) - (15 * liability))\n",
        "  quote = max(quote, 0.01)  # Want positive quotes\n",
        "  stock_data[day] = np.array([quote, day, asset, liability])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Qe2zzN1CESe",
        "outputId": "c0b472e9-8b92-4d3e-ab70-0494a7cc9f92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "stock_data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.00000000e-02, 0.00000000e+00, 3.26720097e-01, 6.07011611e-01],\n",
              "       [4.61851638e+00, 1.00000000e+00, 9.08628802e-01, 9.94584221e-01],\n",
              "       [8.81810725e+00, 2.00000000e+00, 4.46043779e-01, 1.85134983e-01],\n",
              "       [1.00000000e-02, 3.00000000e+00, 3.40065081e-01, 7.24977202e-01],\n",
              "       [1.46898894e+01, 4.00000000e+00, 7.06710967e-01, 2.28084774e-01],\n",
              "       [8.10892702e+00, 5.00000000e+00, 3.74744413e-01, 5.73821372e-02],\n",
              "       [2.71397837e+00, 6.00000000e+00, 3.06737698e-01, 5.29739950e-01],\n",
              "       [1.72556003e+01, 7.00000000e+00, 7.39886517e-01, 3.57630869e-02],\n",
              "       [1.61211008e+01, 8.00000000e+00, 4.51084138e-01, 7.88296750e-02],\n",
              "       [1.18271641e+01, 9.00000000e+00, 4.68753623e-01, 1.34940475e-01],\n",
              "       [1.82575214e+00, 1.00000000e+01, 3.36091665e-01, 9.19268195e-01],\n",
              "       [1.14644202e+01, 1.10000000e+01, 8.34027123e-01, 9.28717967e-01],\n",
              "       [1.01443960e+01, 1.20000000e+01, 4.11945385e-01, 2.42919509e-01],\n",
              "       [8.67296907e+00, 1.30000000e+01, 6.49540048e-01, 4.48705245e-01],\n",
              "       [9.25510182e+00, 1.40000000e+01, 3.74579226e-01, 1.33714227e-01],\n",
              "       [1.00000000e-02, 1.50000000e+01, 2.82979450e-01, 9.27106763e-01],\n",
              "       [1.00000000e-02, 1.60000000e+01, 1.16889648e-01, 7.47695522e-01],\n",
              "       [1.00000000e-02, 1.70000000e+01, 4.67669933e-02, 9.03556181e-01],\n",
              "       [1.35903176e+01, 1.80000000e+01, 3.74192518e-01, 7.16071982e-02],\n",
              "       [1.89750224e+01, 1.90000000e+01, 8.72116741e-02, 6.82072345e-02],\n",
              "       [1.80971978e+01, 2.00000000e+01, 4.13743487e-01, 9.57983263e-02],\n",
              "       [2.17218153e+01, 2.10000000e+01, 9.23639426e-01, 6.79106646e-01],\n",
              "       [1.13485168e+01, 2.20000000e+01, 4.86346624e-01, 8.99388899e-01],\n",
              "       [2.53453389e+01, 2.30000000e+01, 7.41383731e-01, 1.21074396e-02],\n",
              "       [3.44322313e+01, 2.40000000e+01, 7.84200188e-01, 9.66083875e-02],\n",
              "       [2.61965660e+01, 2.50000000e+01, 5.05852005e-01, 4.57531342e-01],\n",
              "       [1.76473775e+01, 2.60000000e+01, 5.13662497e-01, 7.05586726e-01],\n",
              "       [1.73562389e+00, 2.70000000e+01, 6.89480190e-02, 4.04137843e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BzYy4Pb2CLCh"
      },
      "source": [
        "Let's look again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qdBcScz4CIXr",
        "outputId": "52d27854-35c5-4aaf-de3f-5d70d8424363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "stock_quotes = stock_data[:,0]\n",
        "scatter(days, stock_quotes);"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SBXb7dieCO5h"
      },
      "source": [
        "How does our old model do?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7gAxCgy1COnX",
        "outputId": "83dda9aa-c810-490b-f599-5b721be3ebf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "days = np.array(days).reshape(-1, 1)\n",
        "ols_stocks.fit(days, stock_quotes)\n",
        "ols_stocks.score(days, stock_quotes)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24231584870792025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3E94vTFUCax_"
      },
      "source": [
        "Not bad, but can we do better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mCR5GImZCbGz",
        "outputId": "fed81fda-8c80-47d0-834c-9ff4292411c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ols_stocks.fit(stock_data[:,1:], stock_quotes)\n",
        "ols_stocks.score(stock_data[:,1:], stock_quotes)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.840730705790826"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1Qk-jlBCCiKB"
      },
      "source": [
        "Yep - unsurprisingly, the other covariates (assets and liabilities) have info.\n",
        "\n",
        "But, they do worse without the day data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dDcZl7I5Cf5D",
        "outputId": "40b4c783-1519-44f9-d493-da690ba2d1e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ols_stocks.fit(stock_data[:,2:], stock_quotes)\n",
        "ols_stocks.score(stock_data[:,2:], stock_quotes)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5697224879303924"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pnLXlrK8ENjb"
      },
      "source": [
        "## Time series jargon\n",
        "\n",
        "There's a lot of semi-standard language and tricks to talk about this sort of data. [NIST](https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm) has an excellent guidebook, but here are some highlights:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yWUyhnTbcq55"
      },
      "source": [
        "### Moving average\n",
        "\n",
        "Moving average aka rolling average aka running average.\n",
        "\n",
        "Convert a series of data to a series of averages of continguous subsets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "47bHhBSCcvw-",
        "outputId": "bc58984a-ea9e-4708-afae-14bdcda87549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "stock_quotes_rolling = [sum(stock_quotes[i:i+3]) / 3\n",
        "                        for i in range(len(stock_quotes - 2))]\n",
        "stock_quotes_rolling"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.482207874906698,\n",
              " 4.482207874906698,\n",
              " 7.839332206098022,\n",
              " 7.602938797028199,\n",
              " 8.504264919644198,\n",
              " 9.359501890983296,\n",
              " 12.030226471369417,\n",
              " 15.067955060577136,\n",
              " 9.924672346195711,\n",
              " 8.372445486533062,\n",
              " 7.811522763405338,\n",
              " 10.093928405055907,\n",
              " 9.357488951428092,\n",
              " 5.979356962732095,\n",
              " 3.091700606637547,\n",
              " 0.01,\n",
              " 4.536772538780518,\n",
              " 10.858446661956512,\n",
              " 16.8875126066884,\n",
              " 19.598011834437944,\n",
              " 17.05584331506424,\n",
              " 19.471890332576354,\n",
              " 23.70869567848725,\n",
              " 28.658045406133507,\n",
              " 26.092058278055944,\n",
              " 15.193189129355645,\n",
              " 6.4610004645737655,\n",
              " 0.5785412970739919]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "36XvbGhoc186"
      },
      "source": [
        "Pandas has nice series related functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nTNatxtycys_",
        "outputId": "b351b103-28f8-4f05-90fa-4c9bebc2fa5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(stock_quotes)\n",
        "df.rolling(3).mean()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.482208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.482208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.839332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>7.602939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>8.504265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>9.359502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>12.030226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15.067955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>9.924672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>8.372445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>7.811523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>10.093928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>9.357489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>5.979357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3.091701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.010000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4.536773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>10.858447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>16.887513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>19.598012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>17.055843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>19.471890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>23.708696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>28.658045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26.092058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>15.193189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0\n",
              "0         NaN\n",
              "1         NaN\n",
              "2    4.482208\n",
              "3    4.482208\n",
              "4    7.839332\n",
              "5    7.602939\n",
              "6    8.504265\n",
              "7    9.359502\n",
              "8   12.030226\n",
              "9   15.067955\n",
              "10   9.924672\n",
              "11   8.372445\n",
              "12   7.811523\n",
              "13  10.093928\n",
              "14   9.357489\n",
              "15   5.979357\n",
              "16   3.091701\n",
              "17   0.010000\n",
              "18   4.536773\n",
              "19  10.858447\n",
              "20  16.887513\n",
              "21  19.598012\n",
              "22  17.055843\n",
              "23  19.471890\n",
              "24  23.708696\n",
              "25  28.658045\n",
              "26  26.092058\n",
              "27  15.193189"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "os-szg47dgwf"
      },
      "source": [
        "### Forecasting\n",
        "\n",
        "Forecasting - at it's simplest, it just means \"predict the future\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D_qtt6irdj0x",
        "outputId": "d13dca58-f564-4ba1-dd63-0a1d52024c46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ols_stocks.fit(stock_data[:,1:], stock_quotes)\n",
        "ols_stocks.predict([[29, 0.5, 0.5]])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([20.20256334])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fjnQY0trdnHp"
      },
      "source": [
        "One way to predict if you just have the series data is to use the prior observation. This can be pretty good (if you had to pick one feature to model the temperature for tomorrow, the temperature today is a good choice)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bzC4DV9Hdupp",
        "outputId": "99e7b4e9-77b4-4c22-d289-445b5da97e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "temperature = np.array([30 + random() * day\n",
        "                        for day in np.array(range(365)).reshape(-1, 1)])\n",
        "temperature_next = temperature[1:].reshape(-1, 1)\n",
        "temperature_ols = LinearRegression()\n",
        "temperature_ols.fit(temperature[:-1], temperature_next)\n",
        "temperature_ols.score(temperature[:-1], temperature_next)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15585099989418716"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RFdssXQbdxbE"
      },
      "source": [
        "But you can often make it better by considering more than one prior observation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pVfUqD2YdxxZ",
        "outputId": "c5d8e9a6-4f45-452f-b906-331d486d0977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "temperature_next_next = temperature[2:].reshape(-1, 1)\n",
        "temperature_two_past = np.concatenate([temperature[:-2], temperature_next[:-1]],\n",
        "                                      axis=1)\n",
        "temperature_ols.fit(temperature_two_past, temperature_next_next)\n",
        "temperature_ols.score(temperature_two_past, temperature_next_next)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20909100262452962"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c9QltBdmd7TV"
      },
      "source": [
        "### Exponential smoothing\n",
        "\n",
        "Exponential smoothing means using exponentially decreasing past weights to predict the future.\n",
        "\n",
        "You could roll your own, but let's use Pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvMNqunOeC_B",
        "outputId": "84c2dbcf-806a-44f5-f3bc-0dc287257a99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1949
        }
      },
      "source": [
        "temperature_df = pd.DataFrame(temperature)\n",
        "temperature_df.ewm(halflife=7).mean()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30.275729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>30.620143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31.240384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31.087123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>31.730838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>32.404749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>32.124519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>33.027623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>33.347648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>33.202278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>33.053246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>34.037150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>35.128824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>36.076089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>36.857907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>37.145942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>36.741558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>36.731836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>37.055544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>38.333087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>37.942571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>37.731388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>38.945420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>38.090441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>37.874054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>39.657194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>40.739430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>41.590176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>41.806233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>335</th>\n",
              "      <td>191.119771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336</th>\n",
              "      <td>204.648492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>337</th>\n",
              "      <td>203.452122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338</th>\n",
              "      <td>190.857789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>199.856273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>189.237560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>189.794869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>201.709696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>217.355652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>344</th>\n",
              "      <td>201.847527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345</th>\n",
              "      <td>196.062402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>205.024709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>191.802314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>178.257726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>183.161248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>191.952007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351</th>\n",
              "      <td>188.564725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>352</th>\n",
              "      <td>195.571905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>353</th>\n",
              "      <td>207.051103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354</th>\n",
              "      <td>214.698707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355</th>\n",
              "      <td>202.966726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>206.889798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>220.479359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>204.207859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>213.957455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360</th>\n",
              "      <td>200.341299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>211.278898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>211.836011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>213.522922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>214.455090</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>365 rows  1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0\n",
              "0     30.000000\n",
              "1     30.275729\n",
              "2     30.620143\n",
              "3     31.240384\n",
              "4     31.087123\n",
              "5     31.730838\n",
              "6     32.404749\n",
              "7     32.124519\n",
              "8     33.027623\n",
              "9     33.347648\n",
              "10    33.202278\n",
              "11    33.053246\n",
              "12    34.037150\n",
              "13    35.128824\n",
              "14    36.076089\n",
              "15    36.857907\n",
              "16    37.145942\n",
              "17    36.741558\n",
              "18    36.731836\n",
              "19    37.055544\n",
              "20    38.333087\n",
              "21    37.942571\n",
              "22    37.731388\n",
              "23    38.945420\n",
              "24    38.090441\n",
              "25    37.874054\n",
              "26    39.657194\n",
              "27    40.739430\n",
              "28    41.590176\n",
              "29    41.806233\n",
              "..          ...\n",
              "335  191.119771\n",
              "336  204.648492\n",
              "337  203.452122\n",
              "338  190.857789\n",
              "339  199.856273\n",
              "340  189.237560\n",
              "341  189.794869\n",
              "342  201.709696\n",
              "343  217.355652\n",
              "344  201.847527\n",
              "345  196.062402\n",
              "346  205.024709\n",
              "347  191.802314\n",
              "348  178.257726\n",
              "349  183.161248\n",
              "350  191.952007\n",
              "351  188.564725\n",
              "352  195.571905\n",
              "353  207.051103\n",
              "354  214.698707\n",
              "355  202.966726\n",
              "356  206.889798\n",
              "357  220.479359\n",
              "358  204.207859\n",
              "359  213.957455\n",
              "360  200.341299\n",
              "361  211.278898\n",
              "362  211.836011\n",
              "363  213.522922\n",
              "364  214.455090\n",
              "\n",
              "[365 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gBEjBZVbeH6R"
      },
      "source": [
        "Halflife is among the parameters we can play with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HjZgMwYkeODN",
        "outputId": "3f8d03f6-2854-4a8d-88cb-5ab3d68a8ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "sse_1 = ((temperature_df - temperature_df.ewm(halflife=7).mean())**2).sum()\n",
        "sse_2 = ((temperature_df - temperature_df.ewm(halflife=3).mean())**2).sum()\n",
        "print(sse_1)\n",
        "print(sse_2)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    1.241463e+06\n",
            "dtype: float64\n",
            "0    1.013226e+06\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s39bj4g9eQ9Z"
      },
      "source": [
        "Note - the first error being higher doesn't mean it's necessarily *worse*. It's *smoother* as expected, and if that's what we care about - great!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OcPMn8o4eYP1"
      },
      "source": [
        "### Seasonality\n",
        "\n",
        "Seasonality - \"day of week\"-effects, and more. In a lot of real world data, certain time periods are systemically different, e.g. holidays for retailers, weekends for restaurants, seasons for weather.\n",
        "\n",
        "Let's try to make some seasonal data - a store that sells more later in a week:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h0qPMWCreheL",
        "outputId": "ce2a8613-2539-4472-c70c-93b11cec3c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "sales = np.array([random() + (day % 7) * random() for day in days])\n",
        "scatter(days, sales)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f1a7e061400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngGQIbgBIhuAGgGT+D6G59+cKAEwVAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LEADkcMzelxY"
      },
      "source": [
        "How does linear regression do at fitting this?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EV5kt69GenV3",
        "outputId": "4676549d-9dde-42ec-860e-9dbf42870ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sales_ols = LinearRegression()\n",
        "sales_ols.fit(days, sales)\n",
        "sales_ols.score(days, sales)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00047242691432336503"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7shN1eBMep9Q"
      },
      "source": [
        "That's not great - and the fix depends on the domain. Here, we know it'd be best to actually use \"day of week\" as a feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qo9eFlHIeqtA",
        "outputId": "8eab185e-3d5c-4e2e-cce2-3ce27e36a0d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "day_of_week = days % 7\n",
        "sales_ols.fit(day_of_week, sales)\n",
        "sales_ols.score(day_of_week, sales)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5709907169925177"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9ooJIfIMex2G"
      },
      "source": [
        "Note that it's also important to have representative data across whatever seasonal feature(s) you use - don't predict retailers based only on Christmas, as that won't generalize well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "44QZgrPUe3-Y"
      },
      "source": [
        "## Recurrent Neural Networks\n",
        "\n",
        "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
        "\n",
        "$F_n = F_{n-1} + F_{n-2}$\n",
        "\n",
        "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
        "\n",
        "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
        "\n",
        "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
        "\n",
        "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
        "\n",
        "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
        "\n",
        "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
        "\n",
        "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
        "\n",
        "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
        "\n",
        "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
        "\n",
        "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
        "- https://keras.io/layers/recurrent/#lstm\n",
        "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
        "\n",
        "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eWrQllf8WEd-"
      },
      "source": [
        "### RNN/LSTM Sentiment Classification with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ti23G0gRe3kr",
        "outputId": "c29e59c3-dfa2-4710-df37-839e3d8a5f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "'''\n",
        "#Trains an LSTM model on the IMDB sentiment classification task.\n",
        "The dataset is actually too small for LSTM to be of any advantage\n",
        "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
        "**Notes**\n",
        "- RNNs are tricky. Choice of batch size is important,\n",
        "choice of loss and optimizer is critical, etc.\n",
        "Some configurations won't converge.\n",
        "- LSTM loss decrease patterns during training can be quite different\n",
        "from what you see with CNNs/MLPs/etc.\n",
        "'''\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "\n",
        "max_features = 20000\n",
        "# cut texts after this number of words (among top max_features most common words)\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features, allow_pickle = True)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f356e0fca17c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train sequences'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test sequences'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/datasets/imdb.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path, num_words, skip_top, maxlen, seed, start_char, oov_char, index_from, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nb_words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized keyword arguments: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     path = get_file(path,\n",
            "\u001b[0;31mTypeError\u001b[0m: Unrecognized keyword arguments: {'allow_pickle': True}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7pETWPIe362y"
      },
      "source": [
        "### RNN Text generation with NumPy\n",
        "\n",
        "What else can we do with RNN? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. We'll pull some news stories using [newspaper](https://github.com/codelucas/newspaper/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fz1m55G5WSrQ"
      },
      "source": [
        "#### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ahlHBeoZCaLX",
        "outputId": "a5df0fd9-6562-4b8f-c17b-a629664c5b07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        }
      },
      "source": [
        "!pip install newspaper3k"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     || 215kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/90/18ac0e5340b6228c25cc8e79835c3811e7553b2b9ae87296dfeb62b7866d/tldextract-2.2.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     || 51kB 18.2MB/s \n",
            "\u001b[?25hCollecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     || 7.4MB 45.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.21.0)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
            "\u001b[K     || 194kB 41.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.5.3)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2.1->newspaper3k) (1.12.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (41.0.1)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading https://files.pythonhosted.org/packages/23/9c/6e63c23c39e53d3df41c77a3d05a49a42c4e1383a6d2a5e3233161b89dbf/requests_file-1.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow>=3.3.0->newspaper3k) (0.46)\n",
            "Building wheels for collected packages: jieba3k, feedfinder2, feedparser, tinysegmenter\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "Successfully built jieba3k feedfinder2 feedparser tinysegmenter\n",
            "Installing collected packages: requests-file, tldextract, jieba3k, cssselect, feedfinder2, feedparser, tinysegmenter, newspaper3k\n",
            "Successfully installed cssselect-1.0.3 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.4.3 tinysegmenter-0.3 tldextract-2.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fTPlziljCiNJ",
        "colab": {}
      },
      "source": [
        "import newspaper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bk9JF2zaCxoO",
        "outputId": "4be4e6ab-d4ff-4a1d-efef-a181cd526f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ap = newspaper.build('https://www.apnews.com')\n",
        "len(ap.articles)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vc6JgAIJDF4E",
        "outputId": "8bb0b612-72dc-4e85-84cd-8c5d56cb0bf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "article_text = ''\n",
        "\n",
        "for article in ap.articles[:1]:\n",
        "  try:\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    article_text += '\\n\\n' + article.text\n",
        "  except:\n",
        "    print('Failed: ' + article.url)\n",
        "  \n",
        "article_text = article_text.split('\\n\\n')[1]\n",
        "print(article_text)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sorry, this zipcode is not in our deliverable area for this subscription service.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rsMBBMcv_nRM",
        "outputId": "e40085e7-ea6a-423a-d861-1520428be332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Based on \"The Unreasonable Effectiveness of RNN\" implementation\n",
        "import numpy as np\n",
        "\n",
        "chars = list(set(article_text)) # split and remove duplicate characters. convert to list.\n",
        "\n",
        "num_chars = len(chars) # the number of unique characters\n",
        "txt_data_size = len(article_text)\n",
        "\n",
        "print(\"unique characters : \", num_chars)\n",
        "print(\"txt_data_size : \", txt_data_size)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique characters :  23\n",
            "txt_data_size :  81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aQygqc_CAWRA",
        "outputId": "110d1985-1ce1-4297-ebe2-43001de93177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "# one hot encode\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(char_to_int)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(int_to_char)\n",
        "print(\"----------------------------------------------------\")\n",
        "# integer encode input data\n",
        "integer_encoded = [char_to_int[i] for i in article_text] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
        "print(integer_encoded)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"data length : \", len(integer_encoded))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'z': 0, 's': 1, 'v': 2, 'o': 3, ' ': 4, 'h': 5, 'c': 6, 'i': 7, ',': 8, 'y': 9, 't': 10, '.': 11, 'd': 12, 'b': 13, 'u': 14, 'f': 15, 'S': 16, 'e': 17, 'n': 18, 'p': 19, 'r': 20, 'a': 21, 'l': 22}\n",
            "----------------------------------------------------\n",
            "{0: 'z', 1: 's', 2: 'v', 3: 'o', 4: ' ', 5: 'h', 6: 'c', 7: 'i', 8: ',', 9: 'y', 10: 't', 11: '.', 12: 'd', 13: 'b', 14: 'u', 15: 'f', 16: 'S', 17: 'e', 18: 'n', 19: 'p', 20: 'r', 21: 'a', 22: 'l'}\n",
            "----------------------------------------------------\n",
            "[16, 3, 20, 20, 9, 8, 4, 10, 5, 7, 1, 4, 0, 7, 19, 6, 3, 12, 17, 4, 7, 1, 4, 18, 3, 10, 4, 7, 18, 4, 3, 14, 20, 4, 12, 17, 22, 7, 2, 17, 20, 21, 13, 22, 17, 4, 21, 20, 17, 21, 4, 15, 3, 20, 4, 10, 5, 7, 1, 4, 1, 14, 13, 1, 6, 20, 7, 19, 10, 7, 3, 18, 4, 1, 17, 20, 2, 7, 6, 17, 11]\n",
            "----------------------------------------------------\n",
            "data length :  81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bcpMSWDHFowT",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "\n",
        "iteration = 1000\n",
        "sequence_length = 40\n",
        "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
        "hidden_size = 500  # size of hidden layer of neurons.  \n",
        "learning_rate = 1e-1\n",
        "\n",
        "\n",
        "# model parameters\n",
        "\n",
        "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
        "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
        "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
        "\n",
        "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
        "b_y = np.zeros((num_chars, 1)) # output bias\n",
        "\n",
        "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bkqoN86qWaI4"
      },
      "source": [
        "#### Forward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "imfg_Ew0WdDL",
        "colab": {}
      },
      "source": [
        "def forwardprop(inputs, targets, h_prev):\n",
        "        \n",
        "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
        "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
        "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
        "    loss = 0 # loss initialization\n",
        "    \n",
        "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
        "        \n",
        "        xs[t] = np.zeros((num_chars,1)) \n",
        "        xs[t][inputs[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
        "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
        "        \n",
        "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
        "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
        "\n",
        "#         y_class = np.zeros((num_chars, 1)) \n",
        "#         y_class[targets[t]] =1\n",
        "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
        "\n",
        "    return loss, ps, hs, xs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zm6qwNiqWdMe"
      },
      "source": [
        "#### Backward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "81qBiz_xWenI",
        "colab": {}
      },
      "source": [
        "def backprop(ps, inputs, hs, xs, targets):\n",
        "\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
        "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
        "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
        "\n",
        "    # reversed\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
        "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
        "        dWhy += np.dot(dy, hs[t].T)\n",
        "        dby += dy \n",
        "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
        "        dbh += dhraw\n",
        "        dWxh += np.dot(dhraw, xs[t].T)\n",
        "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "        dhnext = np.dot(W_hh.T, dhraw)\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
        "    \n",
        "    return dWxh, dWhh, dWhy, dbh, dby"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r8sBvcdbWfhi"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iA4RM70LWgO_",
        "outputId": "2bb303f4-3529-4067-e990-814328e375c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "data_pointer = 0\n",
        "\n",
        "# memory variables for Adagrad\n",
        "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
        "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
        "\n",
        "for i in range(iteration):\n",
        "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    data_pointer = 0 # go from start of data\n",
        "    \n",
        "    for b in range(batch_size):\n",
        "        \n",
        "        inputs = [char_to_int[ch] for ch in article_text[data_pointer:data_pointer+sequence_length]]\n",
        "        targets = [char_to_int[ch] for ch in article_text[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
        "            \n",
        "        if (data_pointer+sequence_length+1 >= len(article_text) and b == batch_size-1): # processing of the last part of the input data. \n",
        "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
        "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
        "\n",
        "\n",
        "        # forward\n",
        "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
        "#         print(loss)\n",
        "    \n",
        "        # backward\n",
        "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs, targets) \n",
        "        \n",
        "        \n",
        "    # perform parameter update with Adagrad\n",
        "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
        "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "            mem += dparam * dparam # elementwise\n",
        "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
        "    \n",
        "        data_pointer += sequence_length # move data pointer\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss: 4.866431\n",
            "iter 100, loss: 0.000820\n",
            "iter 200, loss: 0.000434\n",
            "iter 300, loss: 0.000313\n",
            "iter 400, loss: 0.000248\n",
            "iter 500, loss: 0.000205\n",
            "iter 600, loss: 0.000166\n",
            "iter 700, loss: 0.000138\n",
            "iter 800, loss: 0.000120\n",
            "iter 900, loss: 0.000106\n",
            "CPU times: user 2min 6s, sys: 1min 6s, total: 3min 13s\n",
            "Wall time: 1min 37s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tjh8Ip68WgYV"
      },
      "source": [
        "#### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HDCxDNPG68Hx",
        "colab": {}
      },
      "source": [
        "def predict(test_char, length):\n",
        "    x = np.zeros((num_chars, 1)) \n",
        "    x[char_to_int[test_char]] = 1\n",
        "    ixes = []\n",
        "    h = np.zeros((hidden_size,1))\n",
        "\n",
        "    for t in range(length):\n",
        "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
        "        y = np.dot(W_hy, h) + b_y\n",
        "        p = np.exp(y) / np.sum(np.exp(y)) \n",
        "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
        "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
        "        x = np.zeros((num_chars, 1)) # init\n",
        "        x[ix] = 1 \n",
        "        ixes.append(ix) # list\n",
        "    txt = test_char + ''.join(int_to_char[i] for i in ixes)\n",
        "    print ('----\\n %s \\n----' % (txt, ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nGVhl-Gxh6N6",
        "outputId": "ae3fb89c-9b96-43ea-da5d-d90c0211136b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "predict('T', 50)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-1f9896870dd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'T'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-c967f94ba143>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(test_char, length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_char\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'T'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xPsz-oefL1kP"
      },
      "source": [
        "Well... that's *vaguely* language-looking. Can you do better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0lfZdD_cp1t5"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "colab": {}
      },
      "source": [
        "\n",
        "import requests\n",
        "import numpy as np\n",
        "r = requests.get('https://www.gutenberg.org/files/100/100-0.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiom7KCpOxE5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "d7176622-1ed3-41c3-d27e-0f58e3ee7bb4"
      },
      "source": [
        "# After some Project Gutenberg preamble, the works begin around character 1500\n",
        "complete_works = r.text[3000:]\n",
        "# First I'll use a smaller subset of the complete works\n",
        "article_text = complete_works[:5000]\n",
        "article_text"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  1\\r\\n\\r\\nFrom fairest creatures we desire increase,\\r\\nThat thereby beautys rose might never die,\\r\\nBut as the riper should by time decease,\\r\\nHis tender heir might bear his memory:\\r\\nBut thou contracted to thine own bright eyes,\\r\\nFeedst thy lights flame with self-substantial fuel,\\r\\nMaking a famine where abundance lies,\\r\\nThy self thy foe, to thy sweet self too cruel:\\r\\nThou that art now the worlds fresh ornament,\\r\\nAnd only herald to the gaudy spring,\\r\\nWithin thine own bud buriest thy content,\\r\\nAnd, tender churl, makst waste in niggarding:\\r\\n  Pity the world, or else this glutton be,\\r\\n  To eat the worlds due, by the grave and thee.\\r\\n\\r\\n\\r\\n                    2\\r\\n\\r\\nWhen forty winters shall besiege thy brow,\\r\\nAnd dig deep trenches in thy beautys field,\\r\\nThy youths proud livery so gazed on now,\\r\\nWill be a tattered weed of small worth held:\\r\\nThen being asked, where all thy beauty lies,\\r\\nWhere all the treasure of thy lusty days;\\r\\nTo say, within thine own deep sunken eyes,\\r\\nWere an all-eating shame, and thriftless praise.\\r\\nHow much more praise deservd thy beautys use,\\r\\nIf thou couldst answer This fair child of mine\\r\\nShall sum my count, and make my old excuse,\\r\\nProving his beauty by succession thine.\\r\\n  This were to be new made when thou art old,\\r\\n  And see thy blood warm when thou feelst it cold.\\r\\n\\r\\n\\r\\n                    3\\r\\n\\r\\nLook in thy glass and tell the face thou viewest,\\r\\nNow is the time that face should form another,\\r\\nWhose fresh repair if now thou not renewest,\\r\\nThou dost beguile the world, unbless some mother.\\r\\nFor where is she so fair whose uneared womb\\r\\nDisdains the tillage of thy husbandry?\\r\\nOr who is he so fond will be the tomb\\r\\nOf his self-love to stop posterity?\\r\\nThou art thy mothers glass and she in thee\\r\\nCalls back the lovely April of her prime,\\r\\nSo thou through windows of thine age shalt see,\\r\\nDespite of wrinkles this thy golden time.\\r\\n  But if thou live remembered not to be,\\r\\n  Die single and thine image dies with thee.\\r\\n\\r\\n\\r\\n                    4\\r\\n\\r\\nUnthrifty loveliness why dost thou spend,\\r\\nUpon thy self thy beautys legacy?\\r\\nNatures bequest gives nothing but doth lend,\\r\\nAnd being frank she lends to those are free:\\r\\nThen beauteous niggard why dost thou abuse,\\r\\nThe bounteous largess given thee to give?\\r\\nProfitless usurer why dost thou use\\r\\nSo great a sum of sums yet canst not live?\\r\\nFor having traffic with thy self alone,\\r\\nThou of thy self thy sweet self dost deceive,\\r\\nThen how when nature calls thee to be gone,\\r\\nWhat acceptable audit canst thou leave?\\r\\n  Thy unused beauty must be tombed with thee,\\r\\n  Which used lives th executor to be.\\r\\n\\r\\n\\r\\n                    5\\r\\n\\r\\nThose hours that with gentle work did frame\\r\\nThe lovely gaze where every eye doth dwell\\r\\nWill play the tyrants to the very same,\\r\\nAnd that unfair which fairly doth excel:\\r\\nFor never-resting time leads summer on\\r\\nTo hideous winter and confounds him there,\\r\\nSap checked with frost and lusty leaves quite gone,\\r\\nBeauty oer-snowed and bareness every where:\\r\\nThen were not summers distillation left\\r\\nA liquid prisoner pent in walls of glass,\\r\\nBeautys effect with beauty were bereft,\\r\\nNor it nor no remembrance what it was.\\r\\n  But flowers distilled though they with winter meet,\\r\\n  Leese but their show, their substance still lives sweet.\\r\\n\\r\\n\\r\\n                    6\\r\\n\\r\\nThen let not winters ragged hand deface,\\r\\nIn thee thy summer ere thou be distilled:\\r\\nMake sweet some vial; treasure thou some place,\\r\\nWith beautys treasure ere it be self-killed:\\r\\nThat use is not forbidden usury,\\r\\nWhich happies those that pay the willing loan;\\r\\nThats for thy self to breed another thee,\\r\\nOr ten times happier be it ten for one,\\r\\nTen times thy self were happier than thou art,\\r\\nIf ten of thine ten times refigured thee:\\r\\nThen what could death do if thou shouldst depart,\\r\\nLeaving thee living in posterity?\\r\\n  Be not self-willed for thou art much too fair,\\r\\n  To be deaths conquest and make worms thine heir.\\r\\n\\r\\n\\r\\n                    7\\r\\n\\r\\nLo in the orient when the gracious light\\r\\nLifts up his burning head, each under eye\\r\\nDoth homage to his new-appearing sight,\\r\\nServing with looks his sacred majesty,\\r\\nAnd having climbed the steep-up heavenly hill,\\r\\nResembling strong youth in his middle age,\\r\\nYet mortal looks adore his beauty still,\\r\\nAttending on his golden pilgrimage:\\r\\nBut when from highmost pitch with weary car,\\r\\nLike feeble age he reeleth from the day,\\r\\nThe eyes (fore duteous) now converted are\\r\\nFrom his low tract and look another way:\\r\\n  So thou, thy self out-going in thy noon:\\r\\n  Unlooked on diest unless thou get a son.\\r\\n\\r\\n\\r\\n                    8\\r\\n\\r\\nMusic to hear, why hearst thou music sadly?\\r\\nSweets with sweets war not, joy delights in joy:\\r\\nWhy lovst thou that which thou receivst not gladly,\\r\\nOr else receivst with pleasure thine annoy?\\r\\nIf the true concord of well-tuned sounds,\\r\\nBy unions married do offend thine ear,\\r\\nThey do but sweetly chide thee, who confounds\\r\\nIn singleness the parts that thou shouldst bear:\\r\\nMark how one string sweet husband to another,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl7dxqDKOzrt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "5e988b37-7d52-4268-e205-c272f946f192"
      },
      "source": [
        "chars = list(set(article_text)) # split and remove duplicate characters. convert to list.\n",
        "num_chars = len(chars) # the number of unique characters\n",
        "txt_data_size = len(article_text)\n",
        "\n",
        "print(\"unique characters : \", num_chars)\n",
        "print(\"txt_data_size : \", txt_data_size)\n",
        "print('All characters: \\n', [x for x in chars])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique characters :  65\n",
            "txt_data_size :  5000\n",
            "All characters: \n",
            " ['z', '3', ':', 's', 'Y', '1', 'v', 'D', ')', '8', ';', 'j', 'W', 'O', 'B', 'R', 'o', '', 'M', ' ', 'h', 'c', '\\n', 'i', ',', '\\r', '-', 'w', 'y', 't', '.', 'A', '', 'g', 'U', 'F', 'd', '2', 'H', 'b', 'L', '6', 'u', '4', '?', 'm', 'x', 'f', 'I', 'q', '5', '7', '(', 'S', 'e', 'T', 'n', 'C', 'p', 'P', 'r', 'a', 'k', 'N', 'l']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5dUjdvtO3rY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "ff6cd2c4-182a-42b8-b898-402c4ed9cabd"
      },
      "source": [
        "# integer-encode\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(char_to_int)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(int_to_char)\n",
        "print(\"----------------------------------------------------\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'z': 0, '3': 1, ':': 2, 's': 3, 'Y': 4, '1': 5, 'v': 6, 'D': 7, ')': 8, '8': 9, ';': 10, 'j': 11, 'W': 12, 'O': 13, 'B': 14, 'R': 15, 'o': 16, '': 17, 'M': 18, ' ': 19, 'h': 20, 'c': 21, '\\n': 22, 'i': 23, ',': 24, '\\r': 25, '-': 26, 'w': 27, 'y': 28, 't': 29, '.': 30, 'A': 31, '': 32, 'g': 33, 'U': 34, 'F': 35, 'd': 36, '2': 37, 'H': 38, 'b': 39, 'L': 40, '6': 41, 'u': 42, '4': 43, '?': 44, 'm': 45, 'x': 46, 'f': 47, 'I': 48, 'q': 49, '5': 50, '7': 51, '(': 52, 'S': 53, 'e': 54, 'T': 55, 'n': 56, 'C': 57, 'p': 58, 'P': 59, 'r': 60, 'a': 61, 'k': 62, 'N': 63, 'l': 64}\n",
            "----------------------------------------------------\n",
            "{0: 'z', 1: '3', 2: ':', 3: 's', 4: 'Y', 5: '1', 6: 'v', 7: 'D', 8: ')', 9: '8', 10: ';', 11: 'j', 12: 'W', 13: 'O', 14: 'B', 15: 'R', 16: 'o', 17: '', 18: 'M', 19: ' ', 20: 'h', 21: 'c', 22: '\\n', 23: 'i', 24: ',', 25: '\\r', 26: '-', 27: 'w', 28: 'y', 29: 't', 30: '.', 31: 'A', 32: '', 33: 'g', 34: 'U', 35: 'F', 36: 'd', 37: '2', 38: 'H', 39: 'b', 40: 'L', 41: '6', 42: 'u', 43: '4', 44: '?', 45: 'm', 46: 'x', 47: 'f', 48: 'I', 49: 'q', 50: '5', 51: '7', 52: '(', 53: 'S', 54: 'e', 55: 'T', 56: 'n', 57: 'C', 58: 'p', 59: 'P', 60: 'r', 61: 'a', 62: 'k', 63: 'N', 64: 'l'}\n",
            "----------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV6A4-2gO6fS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "\n",
        "iteration = 1000\n",
        "sequence_length = 30\n",
        "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
        "hidden_size = 100  # size of hidden layer of neurons.  \n",
        "learning_rate = 1e-1\n",
        "\n",
        "\n",
        "# model parameters\n",
        "\n",
        "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
        "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
        "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
        "\n",
        "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
        "b_y = np.zeros((num_chars, 1)) # output bias\n",
        "\n",
        "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_P3ah1PO9lQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forwardprop(inputs, targets, h_prev):\n",
        "        \n",
        "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
        "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
        "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
        "    loss = 0 # loss initialization\n",
        "    \n",
        "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
        "        \n",
        "        xs[t] = np.zeros((num_chars,1)) \n",
        "        xs[t][inputs[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
        "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
        "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
        " \n",
        "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
        "\n",
        "#         y_class = np.zeros((num_chars, 1)) \n",
        "#         y_class[targets[t]] =1\n",
        "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
        "\n",
        "    return loss, ps, hs, xs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfC43ahAPCuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backprop(ps, inputs, hs, xs):\n",
        "\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
        "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
        "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
        "\n",
        "    # reversed\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
        "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
        "        dWhy += np.dot(dy, hs[t].T)\n",
        "        dby += dy \n",
        "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
        "        dbh += dhraw\n",
        "        dWxh += np.dot(dhraw, xs[t].T)\n",
        "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "        dhnext = np.dot(W_hh.T, dhraw)\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
        "    \n",
        "    return dWxh, dWhh, dWhy, dbh, dby"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX7u7FuXPDxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17385
        },
        "outputId": "3b454612-89bc-4971-e41d-bd196da2e55c"
      },
      "source": [
        "%%time\n",
        "\n",
        "data_pointer = 0\n",
        "\n",
        "# memory variables for Adagrad\n",
        "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
        "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
        "\n",
        "for i in range(iteration):\n",
        "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    data_pointer = 0 # go from start of data\n",
        "    \n",
        "    for b in range(batch_size):\n",
        "        \n",
        "        inputs = [char_to_int[ch] for ch in article_text[data_pointer:data_pointer+sequence_length]]\n",
        "        targets = [char_to_int[ch] for ch in article_text[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
        "            \n",
        "        if (data_pointer+sequence_length+1 >= len(article_text) and b == batch_size-1): # processing of the last part of the input data. \n",
        "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
        "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
        "\n",
        "\n",
        "        # forward\n",
        "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
        "#         print(loss)\n",
        "    \n",
        "        # backward\n",
        "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs) \n",
        "        \n",
        "        \n",
        "    # perform parameter update with Adagrad\n",
        "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
        "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "            mem += dparam * dparam # elementwise\n",
        "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
        "    \n",
        "        data_pointer += sequence_length # move data pointer\n",
        "        \n",
        "    if i % 1 == 0:\n",
        "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss: 69.548286\n",
            "iter 1, loss: 69.218143\n",
            "iter 2, loss: 64.327125\n",
            "iter 3, loss: 61.208547\n",
            "iter 4, loss: 58.372831\n",
            "iter 5, loss: 56.739818\n",
            "iter 6, loss: 55.934622\n",
            "iter 7, loss: 54.584877\n",
            "iter 8, loss: 53.698640\n",
            "iter 9, loss: 57.079825\n",
            "iter 10, loss: 52.199409\n",
            "iter 11, loss: 50.168779\n",
            "iter 12, loss: 49.623851\n",
            "iter 13, loss: 48.994518\n",
            "iter 14, loss: 48.758332\n",
            "iter 15, loss: 48.673976\n",
            "iter 16, loss: 48.077438\n",
            "iter 17, loss: 47.354126\n",
            "iter 18, loss: 46.774290\n",
            "iter 19, loss: 46.506816\n",
            "iter 20, loss: 46.000414\n",
            "iter 21, loss: 45.441282\n",
            "iter 22, loss: 44.735616\n",
            "iter 23, loss: 44.127169\n",
            "iter 24, loss: 43.529001\n",
            "iter 25, loss: 43.031159\n",
            "iter 26, loss: 42.641379\n",
            "iter 27, loss: 42.013346\n",
            "iter 28, loss: 41.738705\n",
            "iter 29, loss: 41.379764\n",
            "iter 30, loss: 41.341419\n",
            "iter 31, loss: 41.126900\n",
            "iter 32, loss: 40.651933\n",
            "iter 33, loss: 40.308148\n",
            "iter 34, loss: 40.467410\n",
            "iter 35, loss: 40.112301\n",
            "iter 36, loss: 40.253481\n",
            "iter 37, loss: 39.297998\n",
            "iter 38, loss: 39.492773\n",
            "iter 39, loss: 39.593316\n",
            "iter 40, loss: 37.485351\n",
            "iter 41, loss: 37.805911\n",
            "iter 42, loss: 36.571156\n",
            "iter 43, loss: 36.660084\n",
            "iter 44, loss: 36.555409\n",
            "iter 45, loss: 37.733003\n",
            "iter 46, loss: 35.470892\n",
            "iter 47, loss: 35.263826\n",
            "iter 48, loss: 34.351642\n",
            "iter 49, loss: 34.702520\n",
            "iter 50, loss: 33.881913\n",
            "iter 51, loss: 34.081977\n",
            "iter 52, loss: 34.339705\n",
            "iter 53, loss: 32.617650\n",
            "iter 54, loss: 32.756209\n",
            "iter 55, loss: 33.675305\n",
            "iter 56, loss: 33.113511\n",
            "iter 57, loss: 31.740565\n",
            "iter 58, loss: 32.344375\n",
            "iter 59, loss: 31.212276\n",
            "iter 60, loss: 31.438767\n",
            "iter 61, loss: 31.785254\n",
            "iter 62, loss: 31.599670\n",
            "iter 63, loss: 33.508158\n",
            "iter 64, loss: 30.781142\n",
            "iter 65, loss: 31.206834\n",
            "iter 66, loss: 31.062951\n",
            "iter 67, loss: 31.793139\n",
            "iter 68, loss: 32.034662\n",
            "iter 69, loss: 29.272243\n",
            "iter 70, loss: 31.550769\n",
            "iter 71, loss: 30.575481\n",
            "iter 72, loss: 30.006754\n",
            "iter 73, loss: 30.056560\n",
            "iter 74, loss: 29.718254\n",
            "iter 75, loss: 29.362639\n",
            "iter 76, loss: 30.288010\n",
            "iter 77, loss: 30.306413\n",
            "iter 78, loss: 28.840216\n",
            "iter 79, loss: 29.528700\n",
            "iter 80, loss: 28.750354\n",
            "iter 81, loss: 29.572529\n",
            "iter 82, loss: 29.097205\n",
            "iter 83, loss: 32.675211\n",
            "iter 84, loss: 28.913074\n",
            "iter 85, loss: 29.462816\n",
            "iter 86, loss: 28.913969\n",
            "iter 87, loss: 28.184720\n",
            "iter 88, loss: 27.380485\n",
            "iter 89, loss: 27.625491\n",
            "iter 90, loss: 29.188814\n",
            "iter 91, loss: 28.700016\n",
            "iter 92, loss: 28.232007\n",
            "iter 93, loss: 27.420590\n",
            "iter 94, loss: 28.398993\n",
            "iter 95, loss: 27.432340\n",
            "iter 96, loss: 26.448021\n",
            "iter 97, loss: 28.817134\n",
            "iter 98, loss: 25.563843\n",
            "iter 99, loss: 26.848726\n",
            "iter 100, loss: 31.486063\n",
            "iter 101, loss: 26.384929\n",
            "iter 102, loss: 28.728994\n",
            "iter 103, loss: 27.927960\n",
            "iter 104, loss: 25.598588\n",
            "iter 105, loss: 26.239452\n",
            "iter 106, loss: 26.177302\n",
            "iter 107, loss: 28.132367\n",
            "iter 108, loss: 26.906440\n",
            "iter 109, loss: 24.961238\n",
            "iter 110, loss: 26.342864\n",
            "iter 111, loss: 28.960861\n",
            "iter 112, loss: 26.002905\n",
            "iter 113, loss: 25.214334\n",
            "iter 114, loss: 24.770882\n",
            "iter 115, loss: 25.336889\n",
            "iter 116, loss: 23.977717\n",
            "iter 117, loss: 24.939700\n",
            "iter 118, loss: 27.319950\n",
            "iter 119, loss: 23.952607\n",
            "iter 120, loss: 27.315998\n",
            "iter 121, loss: 25.751288\n",
            "iter 122, loss: 24.914223\n",
            "iter 123, loss: 23.664686\n",
            "iter 124, loss: 25.913344\n",
            "iter 125, loss: 23.911423\n",
            "iter 126, loss: 25.526199\n",
            "iter 127, loss: 25.131725\n",
            "iter 128, loss: 26.075733\n",
            "iter 129, loss: 24.354067\n",
            "iter 130, loss: 23.023601\n",
            "iter 131, loss: 23.786958\n",
            "iter 132, loss: 22.196362\n",
            "iter 133, loss: 26.329943\n",
            "iter 134, loss: 24.469985\n",
            "iter 135, loss: 23.273564\n",
            "iter 136, loss: 22.166200\n",
            "iter 137, loss: 23.814668\n",
            "iter 138, loss: 22.355393\n",
            "iter 139, loss: 22.593604\n",
            "iter 140, loss: 22.702874\n",
            "iter 141, loss: 21.773352\n",
            "iter 142, loss: 22.601205\n",
            "iter 143, loss: 21.848854\n",
            "iter 144, loss: 21.543548\n",
            "iter 145, loss: 21.640152\n",
            "iter 146, loss: 20.901136\n",
            "iter 147, loss: 21.807667\n",
            "iter 148, loss: 20.883087\n",
            "iter 149, loss: 21.774099\n",
            "iter 150, loss: 20.378493\n",
            "iter 151, loss: 21.907581\n",
            "iter 152, loss: 21.018924\n",
            "iter 153, loss: 21.969628\n",
            "iter 154, loss: 20.585406\n",
            "iter 155, loss: 20.489638\n",
            "iter 156, loss: 21.778547\n",
            "iter 157, loss: 21.174097\n",
            "iter 158, loss: 20.632313\n",
            "iter 159, loss: 20.126271\n",
            "iter 160, loss: 19.652219\n",
            "iter 161, loss: 19.853627\n",
            "iter 162, loss: 21.001572\n",
            "iter 163, loss: 21.165735\n",
            "iter 164, loss: 25.468425\n",
            "iter 165, loss: 22.385378\n",
            "iter 166, loss: 19.425855\n",
            "iter 167, loss: 19.908734\n",
            "iter 168, loss: 20.731411\n",
            "iter 169, loss: 19.039940\n",
            "iter 170, loss: 20.401449\n",
            "iter 171, loss: 22.366134\n",
            "iter 172, loss: 21.930397\n",
            "iter 173, loss: 22.304698\n",
            "iter 174, loss: 20.729406\n",
            "iter 175, loss: 19.754244\n",
            "iter 176, loss: 20.085492\n",
            "iter 177, loss: 20.578619\n",
            "iter 178, loss: 21.919606\n",
            "iter 179, loss: 18.347236\n",
            "iter 180, loss: 20.799412\n",
            "iter 181, loss: 19.326643\n",
            "iter 182, loss: 18.893493\n",
            "iter 183, loss: 18.878623\n",
            "iter 184, loss: 20.677064\n",
            "iter 185, loss: 18.427111\n",
            "iter 186, loss: 19.748615\n",
            "iter 187, loss: 17.708319\n",
            "iter 188, loss: 18.992849\n",
            "iter 189, loss: 19.353354\n",
            "iter 190, loss: 17.910548\n",
            "iter 191, loss: 18.974260\n",
            "iter 192, loss: 18.657258\n",
            "iter 193, loss: 19.789672\n",
            "iter 194, loss: 19.047648\n",
            "iter 195, loss: 19.573892\n",
            "iter 196, loss: 17.008887\n",
            "iter 197, loss: 18.515954\n",
            "iter 198, loss: 17.957680\n",
            "iter 199, loss: 19.564458\n",
            "iter 200, loss: 17.298186\n",
            "iter 201, loss: 19.463350\n",
            "iter 202, loss: 17.969033\n",
            "iter 203, loss: 18.261023\n",
            "iter 204, loss: 18.724576\n",
            "iter 205, loss: 17.424774\n",
            "iter 206, loss: 24.105862\n",
            "iter 207, loss: 17.666583\n",
            "iter 208, loss: 19.804911\n",
            "iter 209, loss: 16.966778\n",
            "iter 210, loss: 18.498110\n",
            "iter 211, loss: 17.149274\n",
            "iter 212, loss: 17.448966\n",
            "iter 213, loss: 19.830520\n",
            "iter 214, loss: 17.444221\n",
            "iter 215, loss: 17.132834\n",
            "iter 216, loss: 17.320272\n",
            "iter 217, loss: 17.344207\n",
            "iter 218, loss: 16.745048\n",
            "iter 219, loss: 16.642337\n",
            "iter 220, loss: 18.027011\n",
            "iter 221, loss: 17.345678\n",
            "iter 222, loss: 18.250416\n",
            "iter 223, loss: 17.323596\n",
            "iter 224, loss: 17.666150\n",
            "iter 225, loss: 16.481319\n",
            "iter 226, loss: 18.046512\n",
            "iter 227, loss: 17.911783\n",
            "iter 228, loss: 16.933599\n",
            "iter 229, loss: 17.007432\n",
            "iter 230, loss: 15.940650\n",
            "iter 231, loss: 18.836042\n",
            "iter 232, loss: 16.719008\n",
            "iter 233, loss: 17.451773\n",
            "iter 234, loss: 16.203946\n",
            "iter 235, loss: 16.622223\n",
            "iter 236, loss: 15.696551\n",
            "iter 237, loss: 19.577301\n",
            "iter 238, loss: 16.511093\n",
            "iter 239, loss: 15.380412\n",
            "iter 240, loss: 17.817941\n",
            "iter 241, loss: 16.329411\n",
            "iter 242, loss: 15.991763\n",
            "iter 243, loss: 16.529376\n",
            "iter 244, loss: 16.273310\n",
            "iter 245, loss: 16.104779\n",
            "iter 246, loss: 15.584168\n",
            "iter 247, loss: 15.731373\n",
            "iter 248, loss: 16.140724\n",
            "iter 249, loss: 16.567104\n",
            "iter 250, loss: 14.838295\n",
            "iter 251, loss: 15.207237\n",
            "iter 252, loss: 16.158862\n",
            "iter 253, loss: 15.977022\n",
            "iter 254, loss: 16.637169\n",
            "iter 255, loss: 15.597158\n",
            "iter 256, loss: 15.395589\n",
            "iter 257, loss: 15.960259\n",
            "iter 258, loss: 16.969290\n",
            "iter 259, loss: 15.983128\n",
            "iter 260, loss: 14.013880\n",
            "iter 261, loss: 15.037298\n",
            "iter 262, loss: 14.548541\n",
            "iter 263, loss: 15.294710\n",
            "iter 264, loss: 15.129904\n",
            "iter 265, loss: 15.450616\n",
            "iter 266, loss: 15.393790\n",
            "iter 267, loss: 14.426054\n",
            "iter 268, loss: 20.915080\n",
            "iter 269, loss: 16.164512\n",
            "iter 270, loss: 14.898595\n",
            "iter 271, loss: 15.398500\n",
            "iter 272, loss: 15.330534\n",
            "iter 273, loss: 15.077087\n",
            "iter 274, loss: 17.805091\n",
            "iter 275, loss: 15.164138\n",
            "iter 276, loss: 14.598872\n",
            "iter 277, loss: 16.795273\n",
            "iter 278, loss: 14.919743\n",
            "iter 279, loss: 15.504142\n",
            "iter 280, loss: 15.966331\n",
            "iter 281, loss: 15.758263\n",
            "iter 282, loss: 14.356008\n",
            "iter 283, loss: 14.823885\n",
            "iter 284, loss: 15.186563\n",
            "iter 285, loss: 14.502105\n",
            "iter 286, loss: 14.646074\n",
            "iter 287, loss: 14.400488\n",
            "iter 288, loss: 14.690207\n",
            "iter 289, loss: 14.037909\n",
            "iter 290, loss: 15.959285\n",
            "iter 291, loss: 14.359289\n",
            "iter 292, loss: 14.494520\n",
            "iter 293, loss: 14.374900\n",
            "iter 294, loss: 14.527583\n",
            "iter 295, loss: 14.606978\n",
            "iter 296, loss: 14.120599\n",
            "iter 297, loss: 15.614227\n",
            "iter 298, loss: 14.463832\n",
            "iter 299, loss: 13.841456\n",
            "iter 300, loss: 13.312923\n",
            "iter 301, loss: 13.962573\n",
            "iter 302, loss: 15.006922\n",
            "iter 303, loss: 14.170976\n",
            "iter 304, loss: 13.666436\n",
            "iter 305, loss: 13.871862\n",
            "iter 306, loss: 14.804424\n",
            "iter 307, loss: 15.032593\n",
            "iter 308, loss: 14.681278\n",
            "iter 309, loss: 14.233339\n",
            "iter 310, loss: 13.792992\n",
            "iter 311, loss: 15.097265\n",
            "iter 312, loss: 13.322617\n",
            "iter 313, loss: 14.608223\n",
            "iter 314, loss: 13.930180\n",
            "iter 315, loss: 13.685107\n",
            "iter 316, loss: 13.444977\n",
            "iter 317, loss: 14.287356\n",
            "iter 318, loss: 14.025313\n",
            "iter 319, loss: 13.651232\n",
            "iter 320, loss: 15.073699\n",
            "iter 321, loss: 13.577258\n",
            "iter 322, loss: 14.409570\n",
            "iter 323, loss: 14.517913\n",
            "iter 324, loss: 12.804076\n",
            "iter 325, loss: 13.738023\n",
            "iter 326, loss: 14.218178\n",
            "iter 327, loss: 13.135598\n",
            "iter 328, loss: 13.231344\n",
            "iter 329, loss: 14.043271\n",
            "iter 330, loss: 13.835554\n",
            "iter 331, loss: 13.729587\n",
            "iter 332, loss: 14.659596\n",
            "iter 333, loss: 14.189618\n",
            "iter 334, loss: 14.551556\n",
            "iter 335, loss: 14.014923\n",
            "iter 336, loss: 13.736053\n",
            "iter 337, loss: 13.863968\n",
            "iter 338, loss: 13.696464\n",
            "iter 339, loss: 15.042402\n",
            "iter 340, loss: 13.880499\n",
            "iter 341, loss: 13.237600\n",
            "iter 342, loss: 14.472622\n",
            "iter 343, loss: 13.618485\n",
            "iter 344, loss: 13.287311\n",
            "iter 345, loss: 13.543987\n",
            "iter 346, loss: 13.543716\n",
            "iter 347, loss: 14.325734\n",
            "iter 348, loss: 14.037363\n",
            "iter 349, loss: 13.140711\n",
            "iter 350, loss: 13.390760\n",
            "iter 351, loss: 12.978638\n",
            "iter 352, loss: 13.877143\n",
            "iter 353, loss: 14.330069\n",
            "iter 354, loss: 14.124173\n",
            "iter 355, loss: 13.655507\n",
            "iter 356, loss: 14.329779\n",
            "iter 357, loss: 12.866515\n",
            "iter 358, loss: 12.842609\n",
            "iter 359, loss: 13.407509\n",
            "iter 360, loss: 12.584021\n",
            "iter 361, loss: 13.509746\n",
            "iter 362, loss: 14.387536\n",
            "iter 363, loss: 12.850015\n",
            "iter 364, loss: 12.983061\n",
            "iter 365, loss: 12.814358\n",
            "iter 366, loss: 13.594627\n",
            "iter 367, loss: 13.204677\n",
            "iter 368, loss: 12.940152\n",
            "iter 369, loss: 12.646945\n",
            "iter 370, loss: 12.183690\n",
            "iter 371, loss: 13.078780\n",
            "iter 372, loss: 12.861035\n",
            "iter 373, loss: 12.804651\n",
            "iter 374, loss: 13.251433\n",
            "iter 375, loss: 13.008880\n",
            "iter 376, loss: 12.694378\n",
            "iter 377, loss: 12.816354\n",
            "iter 378, loss: 11.943997\n",
            "iter 379, loss: 12.438780\n",
            "iter 380, loss: 12.425373\n",
            "iter 381, loss: 12.532647\n",
            "iter 382, loss: 12.769853\n",
            "iter 383, loss: 11.853743\n",
            "iter 384, loss: 12.501316\n",
            "iter 385, loss: 11.349611\n",
            "iter 386, loss: 12.281995\n",
            "iter 387, loss: 13.013395\n",
            "iter 388, loss: 12.318940\n",
            "iter 389, loss: 12.496489\n",
            "iter 390, loss: 12.780997\n",
            "iter 391, loss: 12.845828\n",
            "iter 392, loss: 12.372376\n",
            "iter 393, loss: 12.126837\n",
            "iter 394, loss: 12.352712\n",
            "iter 395, loss: 12.389729\n",
            "iter 396, loss: 11.921022\n",
            "iter 397, loss: 12.155091\n",
            "iter 398, loss: 12.025647\n",
            "iter 399, loss: 12.791042\n",
            "iter 400, loss: 13.026326\n",
            "iter 401, loss: 12.450600\n",
            "iter 402, loss: 11.981426\n",
            "iter 403, loss: 13.182612\n",
            "iter 404, loss: 12.457603\n",
            "iter 405, loss: 11.540362\n",
            "iter 406, loss: 12.135245\n",
            "iter 407, loss: 11.792166\n",
            "iter 408, loss: 12.002037\n",
            "iter 409, loss: 12.070708\n",
            "iter 410, loss: 11.592327\n",
            "iter 411, loss: 12.811983\n",
            "iter 412, loss: 11.556361\n",
            "iter 413, loss: 12.200174\n",
            "iter 414, loss: 11.606067\n",
            "iter 415, loss: 12.406388\n",
            "iter 416, loss: 11.354329\n",
            "iter 417, loss: 11.541802\n",
            "iter 418, loss: 11.161333\n",
            "iter 419, loss: 11.881919\n",
            "iter 420, loss: 11.429054\n",
            "iter 421, loss: 12.381912\n",
            "iter 422, loss: 13.028216\n",
            "iter 423, loss: 11.467479\n",
            "iter 424, loss: 12.244384\n",
            "iter 425, loss: 12.047432\n",
            "iter 426, loss: 11.402363\n",
            "iter 427, loss: 11.608691\n",
            "iter 428, loss: 11.745048\n",
            "iter 429, loss: 12.174149\n",
            "iter 430, loss: 11.135619\n",
            "iter 431, loss: 11.260827\n",
            "iter 432, loss: 11.482478\n",
            "iter 433, loss: 11.533618\n",
            "iter 434, loss: 11.392411\n",
            "iter 435, loss: 12.546818\n",
            "iter 436, loss: 11.828887\n",
            "iter 437, loss: 11.253281\n",
            "iter 438, loss: 12.269510\n",
            "iter 439, loss: 12.042433\n",
            "iter 440, loss: 12.079884\n",
            "iter 441, loss: 12.413222\n",
            "iter 442, loss: 11.347955\n",
            "iter 443, loss: 11.862511\n",
            "iter 444, loss: 10.978534\n",
            "iter 445, loss: 11.034313\n",
            "iter 446, loss: 11.554028\n",
            "iter 447, loss: 12.130676\n",
            "iter 448, loss: 12.267126\n",
            "iter 449, loss: 11.881759\n",
            "iter 450, loss: 11.103738\n",
            "iter 451, loss: 12.062512\n",
            "iter 452, loss: 11.120483\n",
            "iter 453, loss: 11.782981\n",
            "iter 454, loss: 12.269411\n",
            "iter 455, loss: 11.675404\n",
            "iter 456, loss: 11.666261\n",
            "iter 457, loss: 11.721915\n",
            "iter 458, loss: 12.806729\n",
            "iter 459, loss: 11.557360\n",
            "iter 460, loss: 11.430456\n",
            "iter 461, loss: 11.224686\n",
            "iter 462, loss: 11.556154\n",
            "iter 463, loss: 11.271406\n",
            "iter 464, loss: 11.154732\n",
            "iter 465, loss: 12.102714\n",
            "iter 466, loss: 12.500978\n",
            "iter 467, loss: 11.555342\n",
            "iter 468, loss: 11.466956\n",
            "iter 469, loss: 11.220398\n",
            "iter 470, loss: 11.142103\n",
            "iter 471, loss: 11.189462\n",
            "iter 472, loss: 11.515932\n",
            "iter 473, loss: 10.844490\n",
            "iter 474, loss: 11.032471\n",
            "iter 475, loss: 12.787363\n",
            "iter 476, loss: 14.457648\n",
            "iter 477, loss: 11.962692\n",
            "iter 478, loss: 11.282724\n",
            "iter 479, loss: 11.134367\n",
            "iter 480, loss: 11.510324\n",
            "iter 481, loss: 11.168304\n",
            "iter 482, loss: 11.360405\n",
            "iter 483, loss: 10.928313\n",
            "iter 484, loss: 10.989343\n",
            "iter 485, loss: 11.180680\n",
            "iter 486, loss: 11.259499\n",
            "iter 487, loss: 11.671810\n",
            "iter 488, loss: 10.956649\n",
            "iter 489, loss: 10.858959\n",
            "iter 490, loss: 11.746412\n",
            "iter 491, loss: 10.990867\n",
            "iter 492, loss: 11.013068\n",
            "iter 493, loss: 10.935281\n",
            "iter 494, loss: 10.662335\n",
            "iter 495, loss: 10.942146\n",
            "iter 496, loss: 11.164074\n",
            "iter 497, loss: 10.687682\n",
            "iter 498, loss: 11.182722\n",
            "iter 499, loss: 14.596444\n",
            "iter 500, loss: 10.925723\n",
            "iter 501, loss: 11.087603\n",
            "iter 502, loss: 11.202479\n",
            "iter 503, loss: 11.034298\n",
            "iter 504, loss: 10.867644\n",
            "iter 505, loss: 10.365425\n",
            "iter 506, loss: 10.740851\n",
            "iter 507, loss: 10.585432\n",
            "iter 508, loss: 11.499863\n",
            "iter 509, loss: 10.524407\n",
            "iter 510, loss: 11.838866\n",
            "iter 511, loss: 10.408624\n",
            "iter 512, loss: 11.302629\n",
            "iter 513, loss: 13.772248\n",
            "iter 514, loss: 10.696700\n",
            "iter 515, loss: 10.451995\n",
            "iter 516, loss: 11.332707\n",
            "iter 517, loss: 11.193662\n",
            "iter 518, loss: 10.458283\n",
            "iter 519, loss: 10.589472\n",
            "iter 520, loss: 11.189536\n",
            "iter 521, loss: 11.501949\n",
            "iter 522, loss: 10.727377\n",
            "iter 523, loss: 11.670794\n",
            "iter 524, loss: 10.343754\n",
            "iter 525, loss: 11.277520\n",
            "iter 526, loss: 10.266243\n",
            "iter 527, loss: 10.632590\n",
            "iter 528, loss: 10.079438\n",
            "iter 529, loss: 10.443033\n",
            "iter 530, loss: 10.127746\n",
            "iter 531, loss: 9.933629\n",
            "iter 532, loss: 10.413573\n",
            "iter 533, loss: 10.701546\n",
            "iter 534, loss: 10.467829\n",
            "iter 535, loss: 10.318001\n",
            "iter 536, loss: 10.801171\n",
            "iter 537, loss: 11.687318\n",
            "iter 538, loss: 10.499841\n",
            "iter 539, loss: 10.329344\n",
            "iter 540, loss: 10.417738\n",
            "iter 541, loss: 9.844042\n",
            "iter 542, loss: 10.053529\n",
            "iter 543, loss: 10.384707\n",
            "iter 544, loss: 10.280148\n",
            "iter 545, loss: 10.919089\n",
            "iter 546, loss: 10.566173\n",
            "iter 547, loss: 10.439478\n",
            "iter 548, loss: 10.477449\n",
            "iter 549, loss: 10.613212\n",
            "iter 550, loss: 10.494974\n",
            "iter 551, loss: 13.065403\n",
            "iter 552, loss: 11.172138\n",
            "iter 553, loss: 9.849371\n",
            "iter 554, loss: 10.873348\n",
            "iter 555, loss: 9.914635\n",
            "iter 556, loss: 10.696488\n",
            "iter 557, loss: 9.728366\n",
            "iter 558, loss: 9.447781\n",
            "iter 559, loss: 10.072877\n",
            "iter 560, loss: 11.043148\n",
            "iter 561, loss: 10.624895\n",
            "iter 562, loss: 10.989875\n",
            "iter 563, loss: 9.893414\n",
            "iter 564, loss: 10.904536\n",
            "iter 565, loss: 10.260095\n",
            "iter 566, loss: 10.182261\n",
            "iter 567, loss: 9.949281\n",
            "iter 568, loss: 9.926052\n",
            "iter 569, loss: 10.708416\n",
            "iter 570, loss: 11.348539\n",
            "iter 571, loss: 10.579231\n",
            "iter 572, loss: 11.130239\n",
            "iter 573, loss: 10.363893\n",
            "iter 574, loss: 10.099312\n",
            "iter 575, loss: 9.790789\n",
            "iter 576, loss: 10.436319\n",
            "iter 577, loss: 10.257528\n",
            "iter 578, loss: 11.063840\n",
            "iter 579, loss: 10.182991\n",
            "iter 580, loss: 9.622941\n",
            "iter 581, loss: 10.076413\n",
            "iter 582, loss: 10.473306\n",
            "iter 583, loss: 10.951641\n",
            "iter 584, loss: 11.127635\n",
            "iter 585, loss: 9.325300\n",
            "iter 586, loss: 9.704465\n",
            "iter 587, loss: 10.053890\n",
            "iter 588, loss: 10.273404\n",
            "iter 589, loss: 9.292171\n",
            "iter 590, loss: 10.148536\n",
            "iter 591, loss: 9.464304\n",
            "iter 592, loss: 10.012701\n",
            "iter 593, loss: 11.325854\n",
            "iter 594, loss: 10.224214\n",
            "iter 595, loss: 9.642188\n",
            "iter 596, loss: 9.967726\n",
            "iter 597, loss: 10.431693\n",
            "iter 598, loss: 9.523869\n",
            "iter 599, loss: 9.565375\n",
            "iter 600, loss: 9.773907\n",
            "iter 601, loss: 9.506212\n",
            "iter 602, loss: 9.484935\n",
            "iter 603, loss: 10.420951\n",
            "iter 604, loss: 9.926495\n",
            "iter 605, loss: 10.032647\n",
            "iter 606, loss: 9.494797\n",
            "iter 607, loss: 9.420082\n",
            "iter 608, loss: 9.762088\n",
            "iter 609, loss: 9.437481\n",
            "iter 610, loss: 9.171475\n",
            "iter 611, loss: 9.235477\n",
            "iter 612, loss: 10.537903\n",
            "iter 613, loss: 10.419869\n",
            "iter 614, loss: 10.028990\n",
            "iter 615, loss: 9.396680\n",
            "iter 616, loss: 9.604699\n",
            "iter 617, loss: 9.726234\n",
            "iter 618, loss: 9.627470\n",
            "iter 619, loss: 9.541125\n",
            "iter 620, loss: 9.639462\n",
            "iter 621, loss: 9.257887\n",
            "iter 622, loss: 9.385662\n",
            "iter 623, loss: 9.400058\n",
            "iter 624, loss: 8.880792\n",
            "iter 625, loss: 9.315627\n",
            "iter 626, loss: 9.372351\n",
            "iter 627, loss: 11.614507\n",
            "iter 628, loss: 10.106135\n",
            "iter 629, loss: 10.698635\n",
            "iter 630, loss: 10.235147\n",
            "iter 631, loss: 9.383787\n",
            "iter 632, loss: 9.286854\n",
            "iter 633, loss: 8.813643\n",
            "iter 634, loss: 10.209707\n",
            "iter 635, loss: 10.171771\n",
            "iter 636, loss: 9.259595\n",
            "iter 637, loss: 8.821233\n",
            "iter 638, loss: 9.941399\n",
            "iter 639, loss: 9.044326\n",
            "iter 640, loss: 9.088653\n",
            "iter 641, loss: 10.602744\n",
            "iter 642, loss: 9.540338\n",
            "iter 643, loss: 9.277336\n",
            "iter 644, loss: 9.527592\n",
            "iter 645, loss: 8.978283\n",
            "iter 646, loss: 9.384030\n",
            "iter 647, loss: 8.426338\n",
            "iter 648, loss: 8.388648\n",
            "iter 649, loss: 9.096711\n",
            "iter 650, loss: 8.908947\n",
            "iter 651, loss: 9.086315\n",
            "iter 652, loss: 8.819439\n",
            "iter 653, loss: 9.153666\n",
            "iter 654, loss: 8.772254\n",
            "iter 655, loss: 10.847439\n",
            "iter 656, loss: 11.350939\n",
            "iter 657, loss: 9.541439\n",
            "iter 658, loss: 9.496753\n",
            "iter 659, loss: 10.468404\n",
            "iter 660, loss: 9.747966\n",
            "iter 661, loss: 8.631578\n",
            "iter 662, loss: 9.494314\n",
            "iter 663, loss: 8.619778\n",
            "iter 664, loss: 9.069682\n",
            "iter 665, loss: 9.623021\n",
            "iter 666, loss: 9.739042\n",
            "iter 667, loss: 8.569127\n",
            "iter 668, loss: 8.319955\n",
            "iter 669, loss: 8.967600\n",
            "iter 670, loss: 8.726949\n",
            "iter 671, loss: 8.525599\n",
            "iter 672, loss: 9.619669\n",
            "iter 673, loss: 9.062690\n",
            "iter 674, loss: 8.836245\n",
            "iter 675, loss: 9.865404\n",
            "iter 676, loss: 9.114737\n",
            "iter 677, loss: 9.521940\n",
            "iter 678, loss: 8.177191\n",
            "iter 679, loss: 9.700204\n",
            "iter 680, loss: 9.210617\n",
            "iter 681, loss: 11.573005\n",
            "iter 682, loss: 8.860587\n",
            "iter 683, loss: 8.655997\n",
            "iter 684, loss: 8.821628\n",
            "iter 685, loss: 9.387621\n",
            "iter 686, loss: 8.825556\n",
            "iter 687, loss: 8.340800\n",
            "iter 688, loss: 8.157124\n",
            "iter 689, loss: 11.109236\n",
            "iter 690, loss: 9.258579\n",
            "iter 691, loss: 8.604561\n",
            "iter 692, loss: 8.608808\n",
            "iter 693, loss: 8.642412\n",
            "iter 694, loss: 8.150047\n",
            "iter 695, loss: 8.157958\n",
            "iter 696, loss: 9.022451\n",
            "iter 697, loss: 8.737080\n",
            "iter 698, loss: 9.070336\n",
            "iter 699, loss: 9.080469\n",
            "iter 700, loss: 8.749379\n",
            "iter 701, loss: 9.420194\n",
            "iter 702, loss: 8.817700\n",
            "iter 703, loss: 8.649109\n",
            "iter 704, loss: 8.286835\n",
            "iter 705, loss: 8.427968\n",
            "iter 706, loss: 9.248767\n",
            "iter 707, loss: 8.983112\n",
            "iter 708, loss: 8.502724\n",
            "iter 709, loss: 10.081605\n",
            "iter 710, loss: 10.065267\n",
            "iter 711, loss: 9.036262\n",
            "iter 712, loss: 8.968010\n",
            "iter 713, loss: 9.265035\n",
            "iter 714, loss: 7.962791\n",
            "iter 715, loss: 8.758207\n",
            "iter 716, loss: 9.140930\n",
            "iter 717, loss: 8.709800\n",
            "iter 718, loss: 8.603079\n",
            "iter 719, loss: 8.541409\n",
            "iter 720, loss: 8.305231\n",
            "iter 721, loss: 8.352103\n",
            "iter 722, loss: 8.444714\n",
            "iter 723, loss: 8.373611\n",
            "iter 724, loss: 8.063591\n",
            "iter 725, loss: 9.255862\n",
            "iter 726, loss: 9.533279\n",
            "iter 727, loss: 8.330593\n",
            "iter 728, loss: 8.006137\n",
            "iter 729, loss: 8.309130\n",
            "iter 730, loss: 8.038038\n",
            "iter 731, loss: 8.413185\n",
            "iter 732, loss: 8.627308\n",
            "iter 733, loss: 7.982975\n",
            "iter 734, loss: 9.255593\n",
            "iter 735, loss: 8.227893\n",
            "iter 736, loss: 9.276032\n",
            "iter 737, loss: 13.350953\n",
            "iter 738, loss: 9.523687\n",
            "iter 739, loss: 8.855219\n",
            "iter 740, loss: 8.287469\n",
            "iter 741, loss: 7.996308\n",
            "iter 742, loss: 8.400910\n",
            "iter 743, loss: 8.060670\n",
            "iter 744, loss: 7.957989\n",
            "iter 745, loss: 7.769172\n",
            "iter 746, loss: 8.090016\n",
            "iter 747, loss: 9.319268\n",
            "iter 748, loss: 7.841118\n",
            "iter 749, loss: 7.975295\n",
            "iter 750, loss: 8.280662\n",
            "iter 751, loss: 9.332518\n",
            "iter 752, loss: 8.184351\n",
            "iter 753, loss: 8.543323\n",
            "iter 754, loss: 8.920843\n",
            "iter 755, loss: 8.197242\n",
            "iter 756, loss: 8.296071\n",
            "iter 757, loss: 9.037550\n",
            "iter 758, loss: 7.945413\n",
            "iter 759, loss: 7.701365\n",
            "iter 760, loss: 8.239998\n",
            "iter 761, loss: 8.519667\n",
            "iter 762, loss: 7.678760\n",
            "iter 763, loss: 8.190381\n",
            "iter 764, loss: 7.972128\n",
            "iter 765, loss: 8.207029\n",
            "iter 766, loss: 8.461651\n",
            "iter 767, loss: 8.093813\n",
            "iter 768, loss: 8.382075\n",
            "iter 769, loss: 8.339088\n",
            "iter 770, loss: 8.046941\n",
            "iter 771, loss: 7.852405\n",
            "iter 772, loss: 8.111658\n",
            "iter 773, loss: 8.290978\n",
            "iter 774, loss: 8.562279\n",
            "iter 775, loss: 7.772238\n",
            "iter 776, loss: 7.580454\n",
            "iter 777, loss: 7.945068\n",
            "iter 778, loss: 7.905472\n",
            "iter 779, loss: 7.977922\n",
            "iter 780, loss: 8.195428\n",
            "iter 781, loss: 8.383506\n",
            "iter 782, loss: 8.065989\n",
            "iter 783, loss: 7.415108\n",
            "iter 784, loss: 8.037945\n",
            "iter 785, loss: 7.898839\n",
            "iter 786, loss: 7.768665\n",
            "iter 787, loss: 7.647377\n",
            "iter 788, loss: 8.161509\n",
            "iter 789, loss: 7.389295\n",
            "iter 790, loss: 8.062106\n",
            "iter 791, loss: 7.808438\n",
            "iter 792, loss: 7.528408\n",
            "iter 793, loss: 7.906611\n",
            "iter 794, loss: 7.811542\n",
            "iter 795, loss: 8.074967\n",
            "iter 796, loss: 8.061812\n",
            "iter 797, loss: 8.187096\n",
            "iter 798, loss: 8.126548\n",
            "iter 799, loss: 9.605926\n",
            "iter 800, loss: 10.611070\n",
            "iter 801, loss: 8.636691\n",
            "iter 802, loss: 7.841115\n",
            "iter 803, loss: 8.017573\n",
            "iter 804, loss: 7.492435\n",
            "iter 805, loss: 8.438954\n",
            "iter 806, loss: 8.005756\n",
            "iter 807, loss: 9.352863\n",
            "iter 808, loss: 10.528342\n",
            "iter 809, loss: 7.997462\n",
            "iter 810, loss: 8.835559\n",
            "iter 811, loss: 8.901331\n",
            "iter 812, loss: 7.615638\n",
            "iter 813, loss: 7.743028\n",
            "iter 814, loss: 7.764343\n",
            "iter 815, loss: 7.925265\n",
            "iter 816, loss: 7.669074\n",
            "iter 817, loss: 13.215704\n",
            "iter 818, loss: 7.904104\n",
            "iter 819, loss: 7.924441\n",
            "iter 820, loss: 7.831940\n",
            "iter 821, loss: 8.123543\n",
            "iter 822, loss: 7.740283\n",
            "iter 823, loss: 7.850103\n",
            "iter 824, loss: 7.660528\n",
            "iter 825, loss: 7.601783\n",
            "iter 826, loss: 7.324615\n",
            "iter 827, loss: 7.507367\n",
            "iter 828, loss: 7.377622\n",
            "iter 829, loss: 7.486753\n",
            "iter 830, loss: 7.583613\n",
            "iter 831, loss: 7.766810\n",
            "iter 832, loss: 8.156795\n",
            "iter 833, loss: 7.345897\n",
            "iter 834, loss: 8.354084\n",
            "iter 835, loss: 8.022035\n",
            "iter 836, loss: 8.967125\n",
            "iter 837, loss: 7.669765\n",
            "iter 838, loss: 7.500609\n",
            "iter 839, loss: 7.361614\n",
            "iter 840, loss: 7.172312\n",
            "iter 841, loss: 7.476460\n",
            "iter 842, loss: 7.665553\n",
            "iter 843, loss: 7.002434\n",
            "iter 844, loss: 7.340662\n",
            "iter 845, loss: 7.125343\n",
            "iter 846, loss: 7.274274\n",
            "iter 847, loss: 7.214824\n",
            "iter 848, loss: 7.452459\n",
            "iter 849, loss: 7.371953\n",
            "iter 850, loss: 7.198887\n",
            "iter 851, loss: 7.546978\n",
            "iter 852, loss: 8.812306\n",
            "iter 853, loss: 8.173619\n",
            "iter 854, loss: 7.506888\n",
            "iter 855, loss: 7.310675\n",
            "iter 856, loss: 8.484571\n",
            "iter 857, loss: 8.212977\n",
            "iter 858, loss: 7.255496\n",
            "iter 859, loss: 7.736312\n",
            "iter 860, loss: 7.602674\n",
            "iter 861, loss: 7.092272\n",
            "iter 862, loss: 7.313138\n",
            "iter 863, loss: 11.189638\n",
            "iter 864, loss: 7.816405\n",
            "iter 865, loss: 7.546318\n",
            "iter 866, loss: 7.193784\n",
            "iter 867, loss: 7.570958\n",
            "iter 868, loss: 7.581971\n",
            "iter 869, loss: 7.408776\n",
            "iter 870, loss: 7.924986\n",
            "iter 871, loss: 7.610555\n",
            "iter 872, loss: 7.237800\n",
            "iter 873, loss: 7.542971\n",
            "iter 874, loss: 7.328814\n",
            "iter 875, loss: 7.133568\n",
            "iter 876, loss: 7.141831\n",
            "iter 877, loss: 7.303989\n",
            "iter 878, loss: 7.305373\n",
            "iter 879, loss: 7.147854\n",
            "iter 880, loss: 7.175532\n",
            "iter 881, loss: 7.372843\n",
            "iter 882, loss: 7.088086\n",
            "iter 883, loss: 8.029466\n",
            "iter 884, loss: 7.155471\n",
            "iter 885, loss: 7.119691\n",
            "iter 886, loss: 7.746399\n",
            "iter 887, loss: 7.016365\n",
            "iter 888, loss: 8.484968\n",
            "iter 889, loss: 7.617780\n",
            "iter 890, loss: 7.534938\n",
            "iter 891, loss: 7.408741\n",
            "iter 892, loss: 7.431051\n",
            "iter 893, loss: 7.376386\n",
            "iter 894, loss: 7.300615\n",
            "iter 895, loss: 10.741218\n",
            "iter 896, loss: 7.221709\n",
            "iter 897, loss: 7.314579\n",
            "iter 898, loss: 6.916357\n",
            "iter 899, loss: 7.263429\n",
            "iter 900, loss: 7.204200\n",
            "iter 901, loss: 7.561493\n",
            "iter 902, loss: 7.442055\n",
            "iter 903, loss: 7.407244\n",
            "iter 904, loss: 7.051340\n",
            "iter 905, loss: 7.221347\n",
            "iter 906, loss: 7.498181\n",
            "iter 907, loss: 7.162641\n",
            "iter 908, loss: 7.042412\n",
            "iter 909, loss: 7.780468\n",
            "iter 910, loss: 8.003553\n",
            "iter 911, loss: 8.558959\n",
            "iter 912, loss: 7.645264\n",
            "iter 913, loss: 7.432868\n",
            "iter 914, loss: 8.014358\n",
            "iter 915, loss: 8.169240\n",
            "iter 916, loss: 7.982687\n",
            "iter 917, loss: 6.976593\n",
            "iter 918, loss: 7.965411\n",
            "iter 919, loss: 7.609336\n",
            "iter 920, loss: 7.304759\n",
            "iter 921, loss: 6.949429\n",
            "iter 922, loss: 6.922243\n",
            "iter 923, loss: 7.066122\n",
            "iter 924, loss: 8.619041\n",
            "iter 925, loss: 8.131055\n",
            "iter 926, loss: 7.453721\n",
            "iter 927, loss: 7.140622\n",
            "iter 928, loss: 7.229776\n",
            "iter 929, loss: 6.938797\n",
            "iter 930, loss: 7.212045\n",
            "iter 931, loss: 6.935720\n",
            "iter 932, loss: 7.136280\n",
            "iter 933, loss: 7.441499\n",
            "iter 934, loss: 6.931212\n",
            "iter 935, loss: 7.358132\n",
            "iter 936, loss: 6.953406\n",
            "iter 937, loss: 7.217157\n",
            "iter 938, loss: 7.583581\n",
            "iter 939, loss: 8.190644\n",
            "iter 940, loss: 13.150047\n",
            "iter 941, loss: 8.114252\n",
            "iter 942, loss: 9.777225\n",
            "iter 943, loss: 8.135091\n",
            "iter 944, loss: 7.035338\n",
            "iter 945, loss: 7.316687\n",
            "iter 946, loss: 6.945503\n",
            "iter 947, loss: 6.985755\n",
            "iter 948, loss: 6.908926\n",
            "iter 949, loss: 6.883979\n",
            "iter 950, loss: 7.058603\n",
            "iter 951, loss: 7.024711\n",
            "iter 952, loss: 6.941673\n",
            "iter 953, loss: 6.753524\n",
            "iter 954, loss: 7.109236\n",
            "iter 955, loss: 7.062593\n",
            "iter 956, loss: 6.628636\n",
            "iter 957, loss: 6.818916\n",
            "iter 958, loss: 8.647586\n",
            "iter 959, loss: 7.477135\n",
            "iter 960, loss: 7.189680\n",
            "iter 961, loss: 6.953428\n",
            "iter 962, loss: 7.401993\n",
            "iter 963, loss: 7.038910\n",
            "iter 964, loss: 6.892141\n",
            "iter 965, loss: 6.747621\n",
            "iter 966, loss: 6.721259\n",
            "iter 967, loss: 6.638311\n",
            "iter 968, loss: 6.671778\n",
            "iter 969, loss: 6.600467\n",
            "iter 970, loss: 6.806893\n",
            "iter 971, loss: 7.126010\n",
            "iter 972, loss: 6.750497\n",
            "iter 973, loss: 7.017587\n",
            "iter 974, loss: 6.490187\n",
            "iter 975, loss: 6.680748\n",
            "iter 976, loss: 6.611069\n",
            "iter 977, loss: 8.675673\n",
            "iter 978, loss: 6.487846\n",
            "iter 979, loss: 8.924561\n",
            "iter 980, loss: 7.166202\n",
            "iter 981, loss: 6.741491\n",
            "iter 982, loss: 6.828909\n",
            "iter 983, loss: 6.740739\n",
            "iter 984, loss: 7.140065\n",
            "iter 985, loss: 6.499015\n",
            "iter 986, loss: 6.771310\n",
            "iter 987, loss: 6.873427\n",
            "iter 988, loss: 6.902341\n",
            "iter 989, loss: 7.152908\n",
            "iter 990, loss: 7.087705\n",
            "iter 991, loss: 8.969646\n",
            "iter 992, loss: 7.022488\n",
            "iter 993, loss: 7.061053\n",
            "iter 994, loss: 6.836506\n",
            "iter 995, loss: 6.619116\n",
            "iter 996, loss: 6.468869\n",
            "iter 997, loss: 6.727711\n",
            "iter 998, loss: 7.205600\n",
            "iter 999, loss: 7.248100\n",
            "CPU times: user 13min 16s, sys: 8min 24s, total: 21min 40s\n",
            "Wall time: 11min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_FR01bePaWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(test_char, length):\n",
        "    x = np.zeros((num_chars, 1)) \n",
        "    x[char_to_int[test_char]] = 1\n",
        "    ixes = []\n",
        "    h = np.zeros((hidden_size,1))\n",
        "\n",
        "    for t in range(length):\n",
        "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
        "        y = np.dot(W_hy, h) + b_y\n",
        "        p = np.exp(y) / np.sum(np.exp(y)) \n",
        "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
        "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
        "        x = np.zeros((num_chars, 1)) # init\n",
        "        x[ix] = 1 \n",
        "        ixes.append(ix) # list\n",
        "    txt = test_char + ''.join(int_to_char[i] for i in ixes)\n",
        "    print ('----\\n %s \\n----' % (txt, ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqG7DehHPhDq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "388d9eab-5360-4a6d-fdc5-d8ee5b2c7799"
      },
      "source": [
        "predict('C', 1000)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " Calenglg thou dotly nethe to thy eacrid:\r\n",
            "Then beetes beact whev an thy beag thoue ane wheme heet,\r\n",
            "Not conver and bue gids loce meaut, soobel,\r\n",
            "Upaoutt hede\rst thes prere the sumf-with wac and the dares ornoter:\r\n",
            "Tot ivert niv noth beprare deent os muedr and the to thines brert shout autey,\r\n",
            "   8ldo to thy lovig:\r\n",
            "Thes withed andecthy beauryed owhrong thou weou what coumers fagures thou shessd,\r\n",
            "Andienshaldst on thee pilgst nos if now lotl summed haves,\r\n",
            "Tond netens nearest feet andst mou eil,\r\n",
            " thides ware trie they conagd thou whound prise to t oo Lot not thy gof minlest cee.\r\n",
            "\r\n",
            "\r\n",
            "  b of thy ble matich pinget,s rie:\r\n",
            "With mert nove and and weilllowh rime sot the sowe miy dot ap ilcare corld sort ou beautess unos theee:\r\n",
            "Whisead anothing his alltlend thou unese grlilld Ap:\r\n",
            "In shes torg auty bys bequing fo,\r\n",
            "His ell swelly:\r\n",
            "Buth swine ones dose chys lothr not tond.\r\n",
            "\r\n",
            "\r\n",
            "                                                                                                               \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}